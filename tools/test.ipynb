{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.03999931e+05  4.31955949e+05 -1.53300000e+00]\n",
      " [ 1.03999966e+05  4.31956816e+05 -1.48600000e+00]\n",
      " [ 1.03999892e+05  4.31956519e+05 -1.47500000e+00]\n",
      " ...\n",
      " [ 1.04090448e+05  4.32111861e+05 -1.52100000e+00]\n",
      " [ 1.04090522e+05  4.32111570e+05 -1.48900000e+00]\n",
      " [ 1.04090538e+05  4.32113186e+05 -2.05200000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as  np\n",
    "\n",
    "loadData = np.load('/home/ExtraData/SceneClass/Data/train_data/farmland/2/Point.npy')\n",
    "print(loadData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "一个pytorch格式的tensor形状为（32，32，32，32），也就是说有32个通道，每个通道的形状为（32，32，32）。我想把各个通道相同位置的元素取出来各形成一个32维向量，存入一个list中，并把其对应的位置存在另一个list中\n",
    "\n",
    "2023/11/19 23:49:40\n",
    "\n",
    "你可以使用以下代码来实现你的需求：\n",
    "\n",
    "python\n",
    "import torch\n",
    "\n",
    "# 创建一个形状为（32，32，32，32）的tensor\n",
    "tensor = torch.randn(32, 32, 32, 32)\n",
    "\n",
    "# 将tensor按照第一个维度拆分成32个tensor\n",
    "channel_tensors = torch.split(tensor, 1, dim=0)\n",
    "\n",
    "# 初始化两个空的list，用来存储向量和位置\n",
    "vectors_list = []\n",
    "positions_list = []\n",
    "\n",
    "# 遍历每个通道的tensor\n",
    "for i, channel_tensor in enumerate(channel_tensors):\n",
    "    # 将每个通道的tensor拉平成一维向量，并转为numpy数组\n",
    "    vector = channel_tensor.view(-1).numpy()\n",
    "    # 将向量存入list\n",
    "    vectors_list.append(vector)\n",
    "    # 存储对应位置\n",
    "    positions_list.append(i)\n",
    "\n",
    "# 打印结果\n",
    "print(vectors_list)\n",
    "print(positions_list)\n",
    "这段代码首先将形状为（32，32，32，32）的tensor按照第一个维度拆分成32个tensor，然后将每个通道的tensor拉平成一维向量并存入一个list中，同时将对应的位置存入另一个list中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 创建一个形状为（32，32，32，32）的随机tensor作为示例\n",
    "tensor = torch.randn(16, 32, 64, 64, 64)\n",
    "\n",
    "vectors_list = []\n",
    "indices_list = []\n",
    "\n",
    "indices = torch.nonzero(tensor)\n",
    "for idx in indices:\n",
    "    i, j, k = idx[2], idx[3], idx[4]\n",
    "    selected_elements = tensor[idx[0], :, idx[2], idx[3], idx[4]]\n",
    "    vectors_list.append(selected_elements)\n",
    "    indices_list.append(torch.Tensor([idx[0], idx[2], idx[3], idx[4]]))\n",
    "f = torch.stack(vectors_list)\n",
    "c = torch.stack(indices_list)\n",
    "\n",
    "c = c.numpy()\n",
    "# 对二维数组进行去重操作，同时返回被删除行的位置\n",
    "unique_arr, index = np.unique(c, axis=0, return_index=True)\n",
    "\n",
    "print(vectors_list)\n",
    "print(indices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个二维tensor\n",
    "tensor = torch.tensor([[0, 0, 0],\n",
    "                      [1, 0, 3],\n",
    "                      [0, 0, 0],\n",
    "                      [0, 0, 0]])\n",
    "\n",
    "# 找出含有非零元素的行号\n",
    "non_zero_rows = torch.nonzero((tensor != 0).any(dim=1)).squeeze()\n",
    "\n",
    "print(non_zero_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个形状为（16，32，64，64，64）的随机五维tensor\n",
    "tensor = torch.ones(16, 32, 64, 64, 64)\n",
    "\n",
    "# 对第二维度进行求和操作，保持其他维度不变\n",
    "sum_tensor = torch.sum(tensor, dim=1)\n",
    "\n",
    "# 输出新的形状为（16，64，64，64）的tensor\n",
    "print(sum_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个五维tensor\n",
    "tensor = torch.ones(16, 32, 64, 64, 64)  # 生成随机的五维tensor\n",
    "\n",
    "non_zero_indices = torch.nonzero((tensor.sum(dim=(1))) != 0).squeeze()\n",
    "value = tensor[non_zero_indices[:, 0], :, non_zero_indices[:, 1], non_zero_indices[:, 2], non_zero_indices[:, 3]]\n",
    "\n",
    "print(non_zero_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.Tensor([[ 0,  0,  0,  0,  0],\n",
    "        [ 5,  6,  7,  5,  9],\n",
    "        [10, 11, 12, 13, 14],\n",
    "        [15, 16, 17, 18, 19]])\n",
    "\n",
    "b=torch.nonzero(tensor[1]==5)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "tensor([], size=(0, 1), dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def remove_duplicate_rows(tensor: torch.Tensor) -> tuple:\n",
    "    \"\"\"\n",
    "    Remove duplicate rows from a 2D PyTorch tensor and return the unique tensor and indices of removed rows.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): A 2D PyTorch tensor.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the unique tensor and a tensor of indices of removed rows.\n",
    "\n",
    "    Example:\n",
    "        >>> import torch\n",
    "        >>> tensor = torch.tensor([[1, 2], [3, 4], [1, 2], [5, 6]])\n",
    "        >>> unique_tensor, removed_indices = remove_duplicate_rows(tensor)\n",
    "        >>> print(unique_tensor)\n",
    "        tensor([[1, 2],\n",
    "                [3, 4],\n",
    "                [5, 6]])\n",
    "        >>> print(removed_indices)\n",
    "        tensor([2])\n",
    "    \"\"\"\n",
    "    assert tensor.dim() == 2, \"Input tensor must be 2D.\"\n",
    "\n",
    "    unique_tensor, inverse_indices = torch.unique(tensor, sorted=True, return_inverse=True, dim=0)\n",
    "    removed_indices = torch.nonzero(inverse_indices >= tensor.size(0), as_tuple=False)\n",
    "\n",
    "    return unique_tensor, removed_indices\n",
    "\n",
    "# Test the function\n",
    "tensor = torch.tensor([[1, 2], [3, 4], [1, 2], [5, 6], [1, 2], [5, 6], [7, 8]])\n",
    "unique_tensor, removed_indices = remove_duplicate_rows(tensor)\n",
    "print(unique_tensor)\n",
    "print(removed_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去重后的数组： [[1 2]\n",
      " [3 4]\n",
      " [5 6]\n",
      " [7 8]]\n",
      "被删除行的位置： [3 5 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 创建二维数组\n",
    "arr = np.array([[7, 8], [1, 2], [3, 4], [1, 2], [5, 6], [1, 2], [5, 6]])\n",
    "\n",
    "# 对二维数组进行去重操作，同时返回被删除行的位置\n",
    "unique_arr, index = np.unique(arr, axis=0, return_index=True, return_counts = True)\n",
    "\n",
    "# 输出去重后的数组和被删除行的位置\n",
    "print(\"去重后的数组：\", unique_arr)\n",
    "print(\"被删除行的位置：\", np.setdiff1d(np.arange(len(arr)), index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([6, 5, 1, 2, 3, 2, 1, 3, 4, 5, 6, 4, 5, 6])\n",
    "unique_values, indices = np.unique(arr, return_index=True)\n",
    "print(unique_values)  # 输出结果不排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去重后的结果： tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "被删除行的位置： tensor([1, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 构造二维张量\n",
    "x = torch.tensor([[1, 2], [3, 4], [1, 2], [5, 6]])\n",
    "\n",
    "# 对二维张量进行去重\n",
    "unique_x, indices = torch.unique(x, return_inverse=True, dim=0)\n",
    "\n",
    "# 打印去重后的结果和被删除行的位置\n",
    "print(\"去重后的结果：\", unique_x)\n",
    "print(\"被删除行的位置：\", torch.nonzero(torch.bincount(indices) == 1).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去重后的结果： tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "被删除行的行号： tensor([1, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 构造二维张量\n",
    "x = torch.tensor([[1, 2], [3, 4], [1, 2], [5, 6]])\n",
    "\n",
    "# 对二维张量进行去重\n",
    "unique_x, indices = torch.unique(x, return_inverse=True, dim=0)\n",
    "\n",
    "# 打印去重后的结果和被删除行的行号\n",
    "print(\"去重后的结果：\", unique_x)\n",
    "deleted_rows = torch.where(torch.bincount(indices) == 1)[0]\n",
    "print(\"被删除行的行号：\", deleted_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tensor:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "Duplicate rows:\n",
      "tensor([0, 2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 二维张量\n",
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [1, 2, 3], [7, 8, 9]])\n",
    "\n",
    "# 找到重复的行并记录行号\n",
    "unique_tensor, indices = torch.unique(tensor, dim=0, return_inverse=True)\n",
    "duplicates = torch.eq(indices, torch.arange(indices.size(0)).unsqueeze(1)).nonzero()[:, 1]\n",
    "\n",
    "# 打印去重后的张量和被删除行的行号\n",
    "print(\"Unique tensor:\")\n",
    "print(unique_tensor)\n",
    "print(\"Duplicate rows:\")\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ExtraData/SceneClass/tools/test.ipynb 单元格 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.31.254/home/ExtraData/SceneClass/tools/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m16\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m32\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.31.254/home/ExtraData/SceneClass/tools/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m tensor \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.31.254/home/ExtraData/SceneClass/tools/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, x \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39mndenumerate(tensor):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.31.254/home/ExtraData/SceneClass/tools/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mprint\u001b[39m(index, x)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 创建一个形状为（32，32，32，32）的随机tensor作为示例\n",
    "tensor = torch.randn(16, 32, 32, 32, 32)\n",
    "\n",
    "\n",
    "for b in range(tensor.size()[0]):\n",
    "    vectors_list = []\n",
    "    indices_list = []\n",
    "    for i in range(tensor.size()[2]):\n",
    "        for j in range(tensor.size()[3]):\n",
    "            for k in range(tensor.size()[4]):\n",
    "                selected_elements = tensor[b, :, i, j, k]\n",
    "                if torch.any(selected_elements != 0):\n",
    "                    vectors_list.append(selected_elements)\n",
    "                    indices_list.append(torch.Tensor([b, i, j, k]))\n",
    "    if b == 0:\n",
    "        # 将列表中的张量堆叠起来\n",
    "        f = torch.stack(vectors_list)\n",
    "        feats = f.clone()\n",
    "        c = torch.stack(indices_list)\n",
    "        coords = c.clone()\n",
    "    else:\n",
    "        # 将列表中的张量堆叠起来\n",
    "        f = torch.stack(vectors_list)\n",
    "        feats = torch.cat((feats, f))\n",
    "        c = torch.stack(indices_list)\n",
    "        coords = torch.cat((coords, c)).type(torch.int)\n",
    "print(feats)\n",
    "print(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.Tensor([0.0])\n",
    "\n",
    "if a != 0:\n",
    "    print('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'unravel_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ExtraData/SceneClass/tools/test.ipynb 单元格 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.31.254/home/ExtraData/SceneClass/tools/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(flattened_tensor\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.31.254/home/ExtraData/SceneClass/tools/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# 使用torch.unravel_index将一维索引转换回原来的三维索引\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.31.254/home/ExtraData/SceneClass/tools/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m indices_3d \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49munravel_index(indices, tensor\u001b[39m.\u001b[39msize())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.31.254/home/ExtraData/SceneClass/tools/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# 遍历每个元素并记录其下标\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.31.254/home/ExtraData/SceneClass/tools/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(indices_3d[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)):\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'unravel_index'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个形状为（16，16，16）的三维tensor\n",
    "tensor = torch.randn(16, 16, 16)\n",
    "\n",
    "# 将三维tensor转换成一维tensor\n",
    "flattened_tensor = tensor.view(-1)\n",
    "\n",
    "# 使用torch.arange生成一组索引\n",
    "indices = torch.arange(flattened_tensor.size(0))\n",
    "\n",
    "# 使用torch.unravel_index将一维索引转换回原来的三维索引\n",
    "indices_3d = torch.unravel_index(indices, tensor.size())\n",
    "\n",
    "# 遍历每个元素并记录其下标\n",
    "for i in range(indices_3d[0].size(0)):\n",
    "    index = (indices_3d[0][i].item(), indices_3d[1][i].item(), indices_3d[2][i].item())\n",
    "    value = tensor[index]\n",
    "    # 在这里可以对当前元素进行需要的操作\n",
    "    print(f\"Index: {index}, Value: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23, 30])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假设你的列表是list_of_tensors\n",
    "list_of_tensors = [torch.randn(23) for _ in range(30)]\n",
    "\n",
    "# 将列表中的张量堆叠起来\n",
    "stacked_tensor = torch.stack(list_of_tensors)\n",
    "\n",
    "# 转置张量\n",
    "reshaped_tensor = torch.transpose(stacked_tensor, 0, 1)\n",
    "\n",
    "print(reshaped_tensor.shape)  # 输出：torch.Size([30, 23])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 假设你的列表是list_of_tensors\n",
    "list_of_tensors = [torch.randn(23) for _ in range(30)]\n",
    "\n",
    "# 将列表中的张量堆叠起来\n",
    "stacked_tensor = torch.stack(list_of_tensors)\n",
    "\n",
    "# 转置张量\n",
    "reshaped_tensor = torch.transpose(stacked_tensor, 0, 1)\n",
    "\n",
    "print(reshaped_tensor.shape)  # 输出：torch.Size([30, 23])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 假设你有两个一维张量 tensor1 和 tensor2\n",
    "tensor1 = torch.tensor([1, 2, 3])\n",
    "tensor2 = torch.tensor([4, 5, 6])\n",
    "\n",
    "# 将两个张量沿着行的维度进行堆叠\n",
    "stacked_tensor = torch.stack((tensor1, tensor2))\n",
    "\n",
    "print(stacked_tensor)\n",
    "print(stacked_tensor.shape)  # 输出：torch.Size([2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1048576])\n",
      "torch.Size([32768, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建形状为（32，32，32，32）的tensor\n",
    "tensor = torch.randn(32, 32, 32, 32)\n",
    "\n",
    "# 按照第一个维度拆分成32个tensor\n",
    "split_tensors = torch.split(tensor, 1, dim=0)\n",
    "\n",
    "# 记录每个tensor在原始tensor中的位置\n",
    "positions = []\n",
    "\n",
    "# 将每个通道的tensor拉平成一维向量并存入一个list中，同时将对应的位置存入另一个list中\n",
    "flattened_tensors = []\n",
    "for i, t in enumerate(split_tensors):\n",
    "    flattened = t.view(-1)\n",
    "    flattened_tensors.append(flattened)\n",
    "    positions.extend([(i, j, k) for j in range(32) for k in range(32)])\n",
    "\n",
    "# 将list转换为tensor\n",
    "flattened_tensors = torch.cat(flattened_tensors)\n",
    "positions = torch.tensor(positions)\n",
    "\n",
    "print(flattened_tensors.shape)\n",
    "print(positions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "a = [0, 1, 2, 3]\n",
    "print(a[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 198, 198])\n",
      "torch.Size([2, 3, 98, 98])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, inc, outc, ks=3, stride=1, dilation=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv3d(inc,\n",
    "                        outc,\n",
    "                        kernel_size=ks,\n",
    "                        dilation=dilation,\n",
    "                        stride=stride),\n",
    "            nn.Conv3d(outc, outc, kernel_size=ks, dilation=dilation,\n",
    "                        stride=1),\n",
    "        )\n",
    "\n",
    "        if inc == outc and stride == 1:\n",
    "            self.downsample = nn.Identity()\n",
    "        else:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv3d(inc, outc, kernel_size=1, dilation=1,\n",
    "                            stride=stride),\n",
    "            )\n",
    "\n",
    "        self.relu = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x) + self.downsample(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "tensor = torch.rand((2, 3, 198, 198))\n",
    "print(tensor.shape)\n",
    "net = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=2)\n",
    "# net = nn.Sequential(\n",
    "#             ResidualBlock(3, 4, ks=3, stride=1, dilation=1), # \n",
    "#             ResidualBlock(4, 4, ks=3, stride=1, dilation=1),\n",
    "#         )\n",
    "out = net(tensor)\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# self.net = nn.Sequential(\n",
    "#             spnn.Conv3d(inc,\n",
    "#                         outc,\n",
    "#                         kernel_size=ks,\n",
    "#                         dilation=dilation,\n",
    "#                         stride=stride),\n",
    "#             spnn.BatchNorm(outc),\n",
    "#             spnn.ReLU(True),\n",
    "#             spnn.Conv3d(outc, outc, kernel_size=ks, dilation=dilation,\n",
    "#                         stride=1),\n",
    "#             spnn.BatchNorm(outc),\n",
    "#         )\n",
    "\n",
    "#         if inc == outc and stride == 1:\n",
    "#             self.downsample = nn.Identity()\n",
    "#         else:\n",
    "#             self.downsample = nn.Sequential(\n",
    "#                 spnn.Conv3d(inc, outc, kernel_size=1, dilation=1,\n",
    "#                             stride=stride),\n",
    "#                 spnn.BatchNorm(outc),\n",
    "#             )\n",
    "\n",
    "#         self.relu = spnn.ReLU(True)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.relu(self.net(x) + self.downsample(x))\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 40, 60])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "rs_img = torch.randn((2, 3, 20, 40, 60))\n",
    "y = torch.mean(rs_img, dim=2)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchsparse\n",
    "import torchsparse.nn as spnn\n",
    "import torchsparse.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = [1,2,3,4,5,6,7,8]\n",
    "feats = torch.Tensor(a)\n",
    "num_points = 100\n",
    "dim = 4\n",
    "feats = torch.randn([num_points, dim]).cuda()\n",
    "coords = torch.randint(0, 10, [num_points, 4]).cuda()\n",
    "coords[:, 0] = 0        # Set batch_idx to 0\n",
    "\n",
    "spatial_range = (1, 10, 10, 10)\n",
    "x = torchsparse.SparseTensor(feats, coords, spatial_range = spatial_range)\n",
    "\n",
    "y = x.dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ExtraData/SceneClass/tools/test.ipynb 单元格 8\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.31.254/home/ExtraData/SceneClass/tools/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# 求第三维的最大值，并返回形状为（2，3，10，10）的张量\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.31.254/home/ExtraData/SceneClass/tools/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m max_values, _ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(tensor, dim\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.31.254/home/ExtraData/SceneClass/tools/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39;49m(max_values)\n",
      "\u001b[1;32m/home/ExtraData/SceneClass/tools/test.ipynb 单元格 8\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.31.254/home/ExtraData/SceneClass/tools/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# 求第三维的最大值，并返回形状为（2，3，10，10）的张量\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.31.254/home/ExtraData/SceneClass/tools/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m max_values, _ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(tensor, dim\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.31.254/home/ExtraData/SceneClass/tools/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39;49m(max_values)\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/home/xurui/anaconda3/envs/scene_cls/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m/home/xurui/anaconda3/envs/scene_cls/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个形状为（2，3，10，10，10）的张量\n",
    "tensor = torch.randn(2, 3, 10, 10, 10)\n",
    "\n",
    "# 求第三维的最大值，并返回形状为（2，3，10，10）的张量\n",
    "max_values, _ = torch.max(tensor, dim=3)\n",
    "print(max_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scene_cls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
